{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DINO: Self Distillation with No Labels\n",
    "\n",
    "This notebook is the implementation for a model that is called DINO, created by the Facebook AI Research Team. DINO is a very useful model that learns image representations using contrastive learning, unlike traditional ViT, DINO is trained in a self supervised manner like Masked Autoencoders and SimCLR. The notebook compasses multiple topics like Dino, Dinov2 as well as using registers in transformers to show how they can help in the model performance.\n",
    "\n",
    "Reference Papers: -\n",
    "* [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/pdf/2104.14294)\n",
    "* [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/pdf/2304.07193)\n",
    "* [VISION TRANSFORMERS NEED REGISTERS](https://arxiv.org/pdf/2309.16588)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DINO v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Dino is a methodoly proposed by researchers at Facebook AI Research, the main problem with how ViT's are traditionally trained is that they are trained using supervised datasets, that is they learn representations for images by using the loss that is created by the final MLP head which is used to predict the class the image belongs to. Although this gives rich representations it severely limits the models capability of learning as it needs a very large amount of labelled data. Transformers that are trained on text, during the pretraining stage train using a self supervised methodology as they simply try to predict the next word based on the previous n number of words in the sequence, since this does not require seperate labels, the transformer can fit on a much larger dataset, as since data does not need to be labelled there is a much larger availability of such data. \n",
    "\n",
    "Dino methodology aims to adress the problem by giving a method that allows the ViT to train in a self supervised manner which is called contrastive learning. Since it no longer needs labelled data, the weakness of needed labelled data no longer exists and just images themselves are enough to be given as input to the model.\n",
    "\n",
    "## 2. Architecture and Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
