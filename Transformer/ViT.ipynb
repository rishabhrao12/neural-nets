{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT: Vision Transformers\n",
    "\n",
    "This notebook is demonstration and explanantion for a vision transformer. This architecture was created to use transformers which were created for language modelling, on images while at the same time changing as little of the transformer architecture as possible. The notebook will have an implementation of the vision transformer and will break down the purpose of each component in the architecture of the vision transformer. Finally it will be trained to perform a simple classification task on the large Food101 dataset.\n",
    "\n",
    "Reference Paper: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "Prerequisites: \n",
    "- Basic understanding of Neural Networks, Transformers\n",
    "\n",
    "\n",
    "## 1. Introduction:\n",
    "\n",
    "A Vision Transformer: commonly abbreviated to ViT is transformer that has been specially designed to be used for images. Since transformers usually work with text data where they take a sequence of tokens as an input, they cannot be directly used for the task of working with images. Vision transformers take the image as a sequence of patches. These patches are then fed through the network and the learned representations can then be used for any downstream task. \n",
    "\n",
    "The ViT is an encoder only transformer, i.e, it is used to give embeddings of the images that are fed to it, these embeddings are then used for any downstream task, like segmentation, classification e.t.c.\n",
    "\n",
    "\n",
    "## 2. Architecture\n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://viso.ai/wp-content/uploads/2021/09/vision-transformer-vit.png\", width=\"50%\"></center>\n",
    "<br>\n",
    "\n",
    "In the image given above we can see the architecture of the ViT. Lets walk through the components and their purpose: -\n",
    "\n",
    "1. **Patch Embedding:** The patch embedding is responsible for taking each patch of the image and giving an n dimensional embedding. This is similar to how words were embedded in the transformer. The patch embedding is such that after training similar patches will be given similar embeddings.\n",
    "\n",
    "2. **Positional Embedding:** just like in text where we needed to give positional encoding to the words so that they knew where they were lying, similarly we must do so in images as well. Spatial information of the patches is very important and must be preserved, this is because when the patches are fed to a transformer, all patches interact with each other in a global manner, this is unlike convolutional neural networks where filters are used to ensure only pixels in a neighborhood interact to give output, due to the nature of transformers as a result we need to have positional embedding as well. We add this positional embedding to the patch embedding result of each patch to give us the final patch embeddings.\n",
    "\n",
    "3. **Transformer Encoder**: the transformer encoder takes in the embeddings and gives an output which are the learned representations of the patches, this contains useful information like the relationships of the patches with each other, like for example what patch is likely to follow a given patch etc. The components of the transformer encoder are as follows: -\n",
    "\n",
    "    * **Multi-Headed Attention**: attention is the backbone of a transform, it gives us useful information of each individual token. Attention is made up of key, query and value. Where query is for example your patch query, key is for all the other patches in the sequence and their product represents how much your patch is related to to other pathces in the sequence. Finally value is also used, value contains useful information about your specific patch. They all have the same sized embeddings. It is called multi headed attention as there are multiple heads working in parallel which give the final result.\n",
    "\n",
    "    * **MLP**: the multi-layer perceptron is simply a linear projection of the results of our multi headed attention, it is useful for various reasons such as introducing non-linearity, extracting useful features e.t.c.\n",
    "\n",
    "    * **Layer Norm**: layer norm is applied to the input before being fed to mlp and attention, it normalises the values across a column ensuring stability during training.\n",
    "\n",
    "4. **MLP Head**: Finally the mlp head is a projection head that works on the result of the transformer as a whole to perform a downstream task for example classification. The MLP head works only on one patch from the entire sequence of patches that is generated as output of the model. This singular patch can be either a CLS token or it can be created by taking a global average pooling of all the patches.\n",
    "\n",
    "5. **CLS Token**: the CLS token is a special token that is the same dimensionality as patches after their embedding and it is prepended to the sequence of patches before they are fed to the transformer, a CLS token does not have a positional encoding. The idea for a CLS token is that it will act as a global information holder and will contain an aggregate result of the entire image after training. In some implementation it is used as input to the MLP head for the final task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "In this section we will implement the ViT, and with the help of comments and some documentation explain the reasoning behind the code. Lets first start off with downloading the dataset and exploring it as well as creating various functions that we need to prepare our data to be used in the ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Exploration and Preparation:\n",
    "\n",
    "For this section our goal is to perform the following: -\n",
    "\n",
    "1. Download Dataset: CIFAR10 used for this implementation\n",
    "2. Create data transformer, loader\n",
    "3. Create function to prepare an image for it to be used as input to model\n",
    "4. Create function to visualise an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhrao/opt/miniconda3/envs/dl/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import DataLoader, random_split \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Trainset Length: 50000\n",
      "Valset Length: 10000\n"
     ]
    }
   ],
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.Resize(size=(IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.49139968, 0.48215827 ,0.44653124), (0.24703233, 0.24348505, 0.26158768)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transformer)\n",
    "valset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transformer)\n",
    "\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valloader = DataLoader(dataset=valset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "print(f\"Trainset Length: {len(trainset)}\")\n",
    "print(f\"Valset Length: {len(valset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 64, 64])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Get one batch of images for testing purposed\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "print(images.shape) # (B, C, H, W) -> Batch, Channel, Height, Width (this convention will be used)\n",
    "print(labels.shape) # (B,) -> Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_img(data):\n",
    "    # data -> (C, H, W)\n",
    "    img = data.detach().numpy().transpose(1, 2, 0) # to take C to last channel\n",
    "    mean = np.array([0.49139968, 0.48215827 ,0.44653124])\n",
    "    std = np.array([0.24703233, 0.24348505, 0.26158768])\n",
    "    img = std*img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 63.5, 63.5, -0.5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(visualise_img(images[1]))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_h = 4\n",
    "patch_w = 4\n",
    "num_patches = (IMG_HEIGHT // patch_h) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to get image patches\n",
    "\n",
    "def get_patches(imgs):\n",
    "    B, C, H, W = imgs.shape\n",
    "    patches = imgs.unfold(2, patch_h, patch_h).unfold(3, patch_w, patch_w)\n",
    "    patches = patches.contiguous().view(B, C, -1, patch_h, patch_w) # (B, C, P, h, w)\n",
    "    patches = patches.permute(0, 2, 1, 3, 4)\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images(imgs):\n",
    "    patches = get_patches(imgs)\n",
    "    B, P, C, h, w = patches.shape\n",
    "    patches_flat = patches.reshape(B, P, C*h*w)\n",
    "    return patches_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input images: torch.Size([64, 3, 64, 64])\n",
      "Input to ViT: torch.Size([64, 256, 48])\n"
     ]
    }
   ],
   "source": [
    "# Running prepare images function: -\n",
    "\n",
    "print(f\"Input images: {images.shape}\")\n",
    "print(f\"Input to ViT: {prepare_images(images).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works perfectly as we need the ViT to get the input of the images as flattened patches. Now we can get started on the implementation of the actual model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Implementation:\n",
    "\n",
    "For this section our goal is to perform the following: -\n",
    "\n",
    "1. Construct the ViT and all its components\n",
    "2. Train the model on our image data\n",
    "3. Evaluate the model performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 48])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = prepare_images(images)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_losses():\n",
    "    \"\"\"Get loss for train and val split\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        loader = trainloader if split == 'train' else valloader\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        k = 0\n",
    "        for images, labels in iter(loader):\n",
    "            patches = prepare_images(images)\n",
    "            cls_patch, loss = model(patches, labels)\n",
    "            losses[k] = loss.item()\n",
    "            k += 1\n",
    "            if k == eval_iters:\n",
    "                break\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"Single head of attention for transformer\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): built on nn.Module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, head_size) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size) # this represents the other patches think of them as tags\n",
    "        self.query = nn.Linear(n_embd, head_size) # this represents the tokens as queries\n",
    "        self.value = nn.Linear(n_embd, head_size) # information about each token\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Since this is self attention k,q,v are all made from x itself\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) / (k.size(-1) ** 0.5)  \n",
    "        wei = F.softmax(wei, dim=-1) # probability of each patch wrt other patches\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v # adds individual information of each token to the interrelated information found\n",
    "        return out\n",
    "    \n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention for transformer\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        assert n_embd % num_heads == 0, f\"Embedding size: {n_embd} not divisible num of heads: {num_heads}\"\n",
    "        self.head_size = n_embd // num_heads\n",
    "        self.heads = nn.ModuleList([Head(n_embd, self.head_size) for _ in range(num_heads)])\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # Multiple heads give out of head_size which when concatenated become -> n_embd\n",
    "        out = self.c_proj(out)\n",
    "        return out\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-Layer Perceptron within Transformer Encoder Block\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer Encoder Block\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadedAttention(n_embd, num_heads)\n",
    "        self.mlp = MLP(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x)) # this is a residual skip connection, it allows gradients to flow back easily to input\n",
    "        x = x + self.mlp(self.ln2(x)) # you can see the fact that input is added to output of attn and mlp in the diagram given\n",
    "        return x\n",
    "\n",
    "class MLPHead(nn.Module):\n",
    "    \"\"\"MLP Projection Head for classification\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4*n_embd, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "def get_position_enc(num_patch, n_embd):\n",
    "    \"\"\"Gives sinusoidal positional encodings\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "    assert n_embd % 2 == 0, f\"{n_embd} should be even for positional encoding\"\n",
    "\n",
    "    y_pos = torch.arange(0, num_patch, dtype=torch.float).unsqueeze(1)\n",
    "    x_pos = torch.arange(0, num_patch, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, n_embd // 2, 2).float() * (-torch.log(torch.tensor(1000.0)) / (n_embd // 2)))\n",
    "\n",
    "    pos_encoding = torch.zeros((num_patch, n_embd))\n",
    "    pos_encoding[:, 0::4] = torch.sin(y_pos * div_term)\n",
    "    pos_encoding[:, 1::4] = torch.cos(y_pos * div_term)\n",
    "    pos_encoding[:, 2::4] = torch.sin(x_pos * div_term)\n",
    "    pos_encoding[:, 3::4] = torch.cos(x_pos * div_term)\n",
    "\n",
    "    return pos_encoding\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"Vision Transfomer\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_patches, n_embd, num_heads, num_enc_blocks, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.patch_embedding = nn.Linear(3*patch_h*patch_w, n_embd) # embeds the patches\n",
    "        self.pos_encoding = get_position_enc(num_patches, n_embd) # adds positional information to patches\n",
    "        self.encoder = nn.Sequential(*[TransformerEncoder(n_embd, num_heads) for _ in range(num_enc_blocks)]) # xL number of encoder blocks\n",
    "        self.mlp_head = MLPHead(n_embd, num_classes) # linear projection head for classification\n",
    "        self.cls_token = nn.Parameter(torch.randn(size=(1, n_embd))) # special token for aggregating global information\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        patch_embd = self.patch_embedding(x)\n",
    "        patch_embd += self.pos_encoding # (B, P, n_embd)\n",
    "        cls_tokens = self.cls_token.expand(size=(64, *self.cls_token.shape)) # (B, 1, n_embd)\n",
    "        tokens = torch.cat([cls_tokens, patch_embd], dim=1) # (B, P+1, n_embd)\n",
    "        representations = self.encoder(tokens) # (B, P+1, n_embd)\n",
    "        cls_patch = representations[:, 0, :] # (B, n_embd)\n",
    "        logits = self.mlp_head(cls_patch) # logits of each class\n",
    "        \n",
    "        if y is None:\n",
    "            return cls_patch, logits # if labels not provided i.e, we are performing inference we want logits\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, y) # performs softmax and then negative log likelihood to find loss\n",
    "            return cls_patch, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 500\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total parameters: 266650\n"
     ]
    }
   ],
   "source": [
    "model = ViT(num_patches, n_embd=80, num_heads=5, num_enc_blocks=3, num_classes=10)\n",
    "optimizer = torch.optim.AdamW(lr=1e-3, params=model.parameters())\n",
    "curr_iter = 0\n",
    "losses = []\n",
    "print(f\"Number of total parameters: {sum([torch.numel(p) for p in model.parameters()])}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Train Loss: 2.3341 Val Loss: 2.3394 Time Taken: 33.352516174316406 seconds\n",
      "Step 500: Train Loss: 1.4766 Val Loss: 1.4751 Time Taken: 36.70573306083679 seconds\n",
      "Step 1000: Train Loss: 1.3068 Val Loss: 1.3523 Time Taken: 37.27702522277832 seconds\n",
      "Step 1500: Train Loss: 1.1625 Val Loss: 1.2234 Time Taken: 38.03956198692322 seconds\n",
      "Step 2000: Train Loss: 1.1072 Val Loss: 1.1718 Time Taken: 38.41781306266785 seconds\n",
      "Step 2500: Train Loss: 1.0476 Val Loss: 1.1324 Time Taken: 39.27655386924744 seconds\n",
      "Step 3000: Train Loss: 0.9879 Val Loss: 1.0955 Time Taken: 38.98239612579346 seconds\n",
      "Step 3500: Train Loss: 0.9900 Val Loss: 1.0808 Time Taken: 37.32664608955383 seconds\n"
     ]
    }
   ],
   "source": [
    "# To train the model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in iter(trainloader):\n",
    "        patches = prepare_images(images) # Get patched images\n",
    "        cls_patch, loss = model(patches, labels) # Get cls patch and loss\n",
    "\n",
    "        if curr_iter % eval_interval == 0:\n",
    "            t1 = time()\n",
    "            losses_split = get_losses() # Get loss on train and validation set\n",
    "            t2 = time()\n",
    "            print(f\"Step {curr_iter}: Train Loss: {losses_split['train']:.4f} Val Loss: {losses_split['val']:.4f} Time Taken: {t2-t1} seconds\")\n",
    "            \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        curr_iter += 1\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, '../SavedModels/vit.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../SavedModels/vit.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Evaluation\n",
    "\n",
    "We will check classification report to see how good the model is at learning, and use metrics like classwise precision, accuracy, recall and the f1 score to see how well our model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.33      0.50         6\n",
      "           1       0.50      1.00      0.67         3\n",
      "           2       0.20      0.50      0.29         2\n",
      "           3       0.50      0.50      0.50         6\n",
      "           4       0.38      0.50      0.43         6\n",
      "           5       0.50      0.43      0.46         7\n",
      "           6       0.82      0.82      0.82        11\n",
      "           7       1.00      0.57      0.73         7\n",
      "           8       0.75      0.86      0.80         7\n",
      "           9       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.66        64\n",
      "   macro avg       0.66      0.64      0.61        64\n",
      "weighted avg       0.73      0.66      0.66        64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for images, labels in iter(valloader):\n",
    "    patches = prepare_images(images) # Get patched images\n",
    "    cls_patch, logits = model(patches) # Get cls logits and loss\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    preds = probs.argmax(dim=-1)\n",
    "    print(classification_report(labels, preds))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding and Exploring Model Predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
