{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT: Vision Transformers\n",
    "\n",
    "This notebook is demonstration and explanantion for a vision transformer. This architecture was created to use transformers which were created for language modelling, on images while at the same time changing as little of the transformer architecture as possible. The notebook will have an implementation of the vision transformer and will break down the purpose of each component in the architecture of the vision transformer. Finally it will be trained to perform a simple classification task on the large Food101 dataset.\n",
    "\n",
    "Reference Paper: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "Prerequisites: \n",
    "- Basic understanding of Neural Networks, Transformers\n",
    "\n",
    "\n",
    "## 1. Introduction:\n",
    "\n",
    "A Vision Transformer: commonly abbreviated to ViT is transformer that has been specially designed to be used for images. Since transformers usually work with text data where they take a sequence of tokens as an input, they cannot be directly used for the task of working with images. Vision transformers take the image as a sequence of patches. These patches are then fed through the network and the learned representations can then be used for any downstream task. \n",
    "\n",
    "The ViT is an encoder only transformer, i.e, it is used to give embeddings of the images that are fed to it, these embeddings are then used for any downstream task, like segmentation, classification e.t.c.\n",
    "\n",
    "\n",
    "## 2. Architecture\n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://viso.ai/wp-content/uploads/2021/09/vision-transformer-vit.png\", width=\"50%\"></center>\n",
    "<br>\n",
    "\n",
    "In the image given above we can see the architecture of the ViT. Lets walk through the components and their purpose: -\n",
    "\n",
    "1. **Patch Embedding:** The patch embedding is responsible for taking each patch of the image and giving an n dimensional embedding. This is similar to how words were embedded in the transformer. The patch embedding is such that after training similar patches will be given similar embeddings.\n",
    "\n",
    "2. **Positional Embedding:** just like in text where we needed to give positional encoding to the words so that they knew where they were lying, similarly we must do so in images as well. Spatial information of the patches is very important and must be preserved, this is because when the patches are fed to a transformer, all patches interact with each other in a global manner, this is unlike convolutional neural networks where filters are used to ensure only pixels in a neighborhood interact to give output, due to the nature of transformers as a result we need to have positional embedding as well. We add this positional embedding to the patch embedding result of each patch to give us the final patch embeddings.\n",
    "\n",
    "3. **Transformer Encoder**: the transformer encoder takes in the embeddings and gives an output which are the learned representations of the patches, this contains useful information like the relationships of the patches with each other, like for example what patch is likely to follow a given patch etc. The components of the transformer encoder are as follows: -\n",
    "\n",
    "    * **Multi-Headed Attention**: attention is the backbone of a transform, it gives us useful information of each individual token. Attention is made up of key, query and value. Where query is for example your patch query, key is for all the other patches in the sequence and their product represents how much your patch is related to to other pathces in the sequence. Finally value is also used, value contains useful information about your specific patch. They all have the same sized embeddings. It is called multi headed attention as there are multiple heads working in parallel which give the final result.\n",
    "\n",
    "    * **MLP**: the multi-layer perceptron is simply a linear projection of the results of our multi headed attention, it is useful for various reasons such as introducing non-linearity, extracting useful features e.t.c.\n",
    "\n",
    "    * **Layer Norm**: layer norm is applied to the input before being fed to mlp and attention, it normalises the values across a column ensuring stability during training.\n",
    "\n",
    "4. **MLP Head**: Finally the mlp head is a projection head that works on the result of the transformer as a whole to perform a downstream task for example classification. The MLP head works only on one patch from the entire sequence of patches that is generated as output of the model. This singular patch can be either a CLS token or it can be created by taking a global average pooling of all the patches.\n",
    "\n",
    "5. **CLS Token**: the CLS token is a special token that is the same dimensionality as patches after their embedding and it is prepended to the sequence of patches before they are fed to the transformer, a CLS token does not have a positional encoding. The idea for a CLS token is that it will act as a global information holder and will contain an aggregate result of the entire image after training. In some implementation it is used as input to the MLP head for the final task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "In this section we will implement the ViT, and with the help of comments and some documentation explain the reasoning behind the code. Lets first start off with downloading the dataset and exploring it as well as creating various functions that we need to prepare our data to be used in the ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Exploration and Preparation:\n",
    "\n",
    "For this section our goal is to perform the following: -\n",
    "\n",
    "1. Download Dataset: CIFAR10 used for this implementation\n",
    "2. Create data transformer, loader\n",
    "3. Create function to prepare an image for it to be used as input to model\n",
    "4. Create function to visualise an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import DataLoader, random_split \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Trainset Length: 50000\n",
      "Valset Length: 10000\n"
     ]
    }
   ],
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.Resize(size=(IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.49139968, 0.48215827 ,0.44653124), (0.24703233, 0.24348505, 0.26158768)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transformer)\n",
    "valset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transformer)\n",
    "\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valloader = DataLoader(dataset=valset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "print(f\"Trainset Length: {len(trainset)}\")\n",
    "print(f\"Valset Length: {len(valset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 64, 64])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Get one batch of images for testing purposed\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "print(images.shape) # (B, C, H, W) -> Batch, Channel, Height, Width (this convention will be used)\n",
    "print(labels.shape) # (B,) -> Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_img(data):\n",
    "    # data -> (C, H, W)\n",
    "    img = data.detach().numpy().transpose(1, 2, 0) # to take C to last channel\n",
    "    mean = np.array([0.49139968, 0.48215827 ,0.44653124])\n",
    "    std = np.array([0.24703233, 0.24348505, 0.26158768])\n",
    "    img = std*img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 63.5, 63.5, -0.5)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(visualise_img(images[1]))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_h = 4\n",
    "patch_w = 4\n",
    "num_patches = (IMG_HEIGHT // patch_h) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to get image patches\n",
    "\n",
    "def get_patches(imgs):\n",
    "    B, C, H, W = imgs.shape\n",
    "    patches = imgs.unfold(2, patch_h, patch_h).unfold(3, patch_w, patch_w)\n",
    "    patches = patches.contiguous().view(B, C, -1, patch_h, patch_w) # (B, C, P, h, w)\n",
    "    patches = patches.permute(0, 2, 1, 3, 4)\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images(imgs):\n",
    "    patches = get_patches(imgs)\n",
    "    B, P, C, h, w = patches.shape\n",
    "    patches_flat = patches.reshape(B, P, C*h*w)\n",
    "    return patches_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input images: torch.Size([64, 3, 64, 64])\n",
      "Input to ViT: torch.Size([64, 256, 48])\n"
     ]
    }
   ],
   "source": [
    "# Running prepare images function: -\n",
    "\n",
    "print(f\"Input images: {images.shape}\")\n",
    "print(f\"Input to ViT: {prepare_images(images).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works perfectly as we need the ViT to get the input of the images as flattened patches. Now we can get started on the implementation of the actual model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Implementation:\n",
    "\n",
    "For this section our goal is to perform the following: -\n",
    "\n",
    "1. Construct the ViT and all its components\n",
    "2. Train the model on our image data\n",
    "3. Evaluate the model performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 48])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = prepare_images(images)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # tells torch that computation graph does not need to be maintained for following operations as there will be no backprop\n",
    "def get_losses():\n",
    "    \"\"\"Get loss for train and val split\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    model.eval() # put model in evaluation mode\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        loader = trainloader if split == 'train' else valloader\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        k = 0\n",
    "        for images, labels in iter(loader):\n",
    "            patches = prepare_images(images)\n",
    "            cls_patch, loss = model(patches, labels)\n",
    "            losses[k] = loss.item()\n",
    "            k += 1\n",
    "            if k == eval_iters:\n",
    "                break\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # model needs to be put back into training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q) What is attention?**\n",
    "\n",
    "Ans) Attention is an important concept that forms the very backbone of the transformer, it is the reason transformers perform so well at languge modelling tasks and even at image tasks. Attention gives us the important relationships between the tokens in a sequence, which in this case is the patches of an image. Attention has multiple components, the query, the key and the value.\n",
    "\n",
    "The query can be likened to a term we are trying to search to get related results, this can be thought of as 1 current patch, the key is likened to all the patches in the sequence including the patch in the query itself. The product between these two results will give high values for patches that are related and low for those that are not. The value then is information about a particular token for example how red is it, does it have lines e.t.c, this is then multiplied with the product of the key and query. These are then our attention weights. The learned weights (values of key, qeury, value matrix) will be fixed but they will give different output depending on the input this is the attention weights.\n",
    "\n",
    "Softmax is used to normalise the values to make it come between 0 and 1, sometimes we also use masking (given in transformer section), this is done to help the model learn to predict even if it is given context of varying length (this is given in more detail in decoder only transformer).\n",
    "\n",
    "**Q) Why use multiple heads of attention?**\n",
    "\n",
    "Ans) The idea for using multiple heads of attention is that different heads of attention will focus on different types of relationships, for example one head might focus on the actual meaning of words in a sentence, while another would focus on the grammar of the words in the sentence. Similarly for images, they would focus on different parts or objects in an image, some might focus on the sun in a background another might focus on the people. These heads also work in parallel and they work on the entire sequence however the difference is that they project it to an embedding of size n_embd // num of heads, this is so that after all the outputs are found, they can be concatenated together to give an output of size n_embd and the shape is thus maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"Single head of attention for transformer\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): built on nn.Module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, head_size) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size) # this represents the other patches think of them as tags\n",
    "        self.query = nn.Linear(n_embd, head_size) # this represents the tokens as queries\n",
    "        self.value = nn.Linear(n_embd, head_size) # information about each token\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Since this is self attention k,q,v are all made from x itself\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) / (k.size(-1) ** 0.5)  \n",
    "        wei = F.softmax(wei, dim=-1) # probability of each patch wrt other patches\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v # adds individual information of each token to the interrelated information found\n",
    "        return out\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention for transformer\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        assert n_embd % num_heads == 0, f\"Embedding size: {n_embd} not divisible num of heads: {num_heads}\"\n",
    "        self.head_size = n_embd // num_heads\n",
    "        self.heads = nn.ModuleList([Head(n_embd, self.head_size) for _ in range(num_heads)])\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # Multiple heads give out of head_size which when concatenated become -> n_embd\n",
    "        out = self.c_proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q) What is the need for a MLP in the transformer block?**\n",
    "\n",
    "Ans) THe MLP in the transformer block is very important, although it simply takes the input of size n_embd and projects it back to the same size in the process it adds non linearity to the learned representations from attention. This is done with the help of the GeLU activation funciton. Adding non linearity helps it learn more complicated relationships and helps the model generalise better to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-Layer Perceptron within Transformer Encoder Block\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q) What does the transformer encoder block do? Why is it useful?**\n",
    "\n",
    "The transformer encoder block is responsible for learning and giving embeddings for the input data, these embeddings are very rich in information as compared to the raw data that is fed to a model. Embeddings that are given as output can be used an input to other models for various tasks such as image segmentation, object recognition etc. The reason being that it has been found that we can pretrain on a large dataset and train an encoder to give us image embeddings and then we can use those embeddings in a downstream task like segmentation, and performance is boosted as compared to simply using the raw input images to the segmentation model. Decoders are used to generate output and sometimes we have transformers that have both encoder as well as decoders, where the output of the encoder is given as input to the decoder.\n",
    "\n",
    "**Q) What is layer norm? Why do we need it?**\n",
    "\n",
    "Layer norm, normalises along the column rather than along the batch, this helps all the values for a particular sample to be in the range of 0 and 1, this helps training be more stable, and the model learns better, depending on implementation layer norm can be done before or after the data is fed to the attention and mlp.\n",
    "\n",
    "**Q) Why is x added to the output of the attention and then the same is done even following output from the mlp?**\n",
    "\n",
    "Ans) This is called a residual-skip connection. Think of the model training as a highway, we have our car that zips down the main highway, this is fast and convinient but you are not able to make any stops and perform your daily tasks of buying groceries. What you could do is get off the highway and go into a lane where you could take your time, complete your tasks and then go back to the highway where you can move on with the journey. Similarly the model goes into a branch where it performs tasks like attention, mlp etc the issue with this is that when backpropogating, the gradient could become very small for the input that was given to that block and it hence cant update properly. Addition distributes gradients equally, so by adding x to the output we are distributing gradient equally to both x and to the block of attention, this way both update properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer Encoder Block\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadedAttention(n_embd, num_heads)\n",
    "        self.mlp = MLP(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x)) # this is a residual skip connection, it allows gradients to flow back easily to input\n",
    "        x = x + self.mlp(self.ln2(x)) # you can see the fact that input is added to output of attn and mlp in the diagram given\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q) What is the MLP head? What is a CLS token?**\n",
    "\n",
    "MLP Head is basically a linear projection that is applied on the output of your transformer, this is task specific and in this case it is implemented to perform classification, the reason it is needed is because we want to perform classification task so that we can see how good a representation our embeddings are of the image, because we need to compute a loss in order to backpropogate and update the network, because we need a loss function this MLP head exists to project embeddings into the number of classes so that we can compute a loss. When using the ViT for getting embeddings of patches, we do not use this head, it serves a purpose only during training. In approaches like Self Supervised Learning where we dont require labels to compute loss, we do not have an MLP head.\n",
    "\n",
    "A CLS token is a special token that is prepended to the sequence of patches it is the same shape as the other patches after embedding. This token is a special learned vector, ie it will have the same value weights for all sequences. The idea is that after compute, the global informaion of the image will aggregate and flow to the CLS token making the CLS patch output the representation for the entire image. The reason this happens is because during training we have made sure that only the CLS patch is given as input to the MLP head, so the model realises it needs to accumalate global information of the image into CLS patch as it is the only patch being given for classification which will give the final loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    \"\"\"MLP Projection Head for classification\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4*n_embd, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q) Why do we need positional encodings?**\n",
    "\n",
    "When we usually work with images, we are working with CNN models, these models have convolutional layers where filters work on neighborhood of pixels and then give on pixel output. So they inherinetly work with pixel location information due to the nature of how filters work, on the other hand attention works in a global manner and so local information is not inherently present because of this we need to add positional information to the patches. There are multiple ways of doing this, sinusoidal encodings are one of them, we use sine becuase it has a range and is periodic so it repeats, initially we stretch the sine function to a large extent and store that in the first index of the positional encoding for a patch, this will just give very general information for where the patch lies, however we then reduce the stretch of the function and as we keep reducing it, it will give more and more specific information. Eg initially it will tell you that patch is in the beginning but not where in the beginning, later positions will say it is the second patch. (this is a very simplified example for more detail check: [youtube link](https://www.youtube.com/watch?v=1biZfFLPRSY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_enc(num_patch, n_embd):\n",
    "    \"\"\"Gives sinusoidal positional encodings\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "    assert n_embd % 2 == 0, f\"{n_embd} should be even for positional encoding\"\n",
    "\n",
    "    y_pos = torch.arange(0, num_patch, dtype=torch.float).unsqueeze(1)\n",
    "    x_pos = torch.arange(0, num_patch, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, n_embd // 2, 2).float() * (-torch.log(torch.tensor(1000.0)) / (n_embd // 2)))\n",
    "\n",
    "    pos_encoding = torch.zeros((num_patch, n_embd))\n",
    "    pos_encoding[:, 0::4] = torch.sin(y_pos * div_term)\n",
    "    pos_encoding[:, 1::4] = torch.cos(y_pos * div_term)\n",
    "    pos_encoding[:, 2::4] = torch.sin(x_pos * div_term)\n",
    "    pos_encoding[:, 3::4] = torch.cos(x_pos * div_term)\n",
    "\n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can create the ViT, using all the components created above. Explanation for these components is given in architecture and in the above sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"Vision Transfomer\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_patches, n_embd, num_heads, num_enc_blocks, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.patch_embedding = nn.Linear(3*patch_h*patch_w, n_embd) # embeds the patches\n",
    "        self.pos_encoding = get_position_enc(num_patches, n_embd) # adds positional information to patches\n",
    "        self.encoder = nn.Sequential(*[TransformerEncoder(n_embd, num_heads) for _ in range(num_enc_blocks)]) # xL number of encoder blocks\n",
    "        self.mlp_head = MLPHead(n_embd, num_classes) # linear projection head for classification\n",
    "        self.cls_token = nn.Parameter(torch.randn(size=(1, n_embd))) # special token for aggregating global information\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        patch_embd = self.patch_embedding(x)\n",
    "        patch_embd += self.pos_encoding # (B, P, n_embd)\n",
    "        cls_tokens = self.cls_token.expand(size=(64, *self.cls_token.shape)) # (B, 1, n_embd)\n",
    "        tokens = torch.cat([cls_tokens, patch_embd], dim=1) # (B, P+1, n_embd)\n",
    "        representations = self.encoder(tokens) # (B, P+1, n_embd)\n",
    "        cls_patch = representations[:, 0, :] # (B, n_embd)\n",
    "        logits = self.mlp_head(cls_patch) # logits of each class\n",
    "        \n",
    "        if y is None:\n",
    "            return cls_patch, logits # if labels not provided i.e, we are performing inference we want logits\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, y) # performs softmax and then negative log likelihood to find loss\n",
    "            return cls_patch, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all these components together for easy readability: -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"Single head of attention for transformer\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): built on nn.Module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, head_size) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size) # this represents the other patches think of them as tags\n",
    "        self.query = nn.Linear(n_embd, head_size) # this represents the tokens as queries\n",
    "        self.value = nn.Linear(n_embd, head_size) # information about each token\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Since this is self attention k,q,v are all made from x itself\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) / (k.size(-1) ** 0.5)  \n",
    "        wei = F.softmax(wei, dim=-1) # probability of each patch wrt other patches\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v # adds individual information of each token to the interrelated information found\n",
    "        return out\n",
    "    \n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention for transformer\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        assert n_embd % num_heads == 0, f\"Embedding size: {n_embd} not divisible num of heads: {num_heads}\"\n",
    "        self.head_size = n_embd // num_heads\n",
    "        self.heads = nn.ModuleList([Head(n_embd, self.head_size) for _ in range(num_heads)])\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # Multiple heads give out of head_size which when concatenated become -> n_embd\n",
    "        out = self.c_proj(out)\n",
    "        return out\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-Layer Perceptron within Transformer Encoder Block\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer Encoder Block\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadedAttention(n_embd, num_heads)\n",
    "        self.mlp = MLP(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x)) # this is a residual skip connection, it allows gradients to flow back easily to input\n",
    "        x = x + self.mlp(self.ln2(x)) # you can see the fact that input is added to output of attn and mlp in the diagram given\n",
    "        return x\n",
    "\n",
    "class MLPHead(nn.Module):\n",
    "    \"\"\"MLP Projection Head for classification\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4*n_embd, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "def get_position_enc(num_patch, n_embd):\n",
    "    \"\"\"Gives sinusoidal positional encodings\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "    assert n_embd % 2 == 0, f\"{n_embd} should be even for positional encoding\"\n",
    "\n",
    "    y_pos = torch.arange(0, num_patch, dtype=torch.float).unsqueeze(1)\n",
    "    x_pos = torch.arange(0, num_patch, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, n_embd // 2, 2).float() * (-torch.log(torch.tensor(1000.0)) / (n_embd // 2)))\n",
    "\n",
    "    pos_encoding = torch.zeros((num_patch, n_embd))\n",
    "    pos_encoding[:, 0::4] = torch.sin(y_pos * div_term)\n",
    "    pos_encoding[:, 1::4] = torch.cos(y_pos * div_term)\n",
    "    pos_encoding[:, 2::4] = torch.sin(x_pos * div_term)\n",
    "    pos_encoding[:, 3::4] = torch.cos(x_pos * div_term)\n",
    "\n",
    "    return pos_encoding\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"Vision Transfomer\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_patches, n_embd, num_heads, num_enc_blocks, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.patch_embedding = nn.Linear(3*patch_h*patch_w, n_embd) # embeds the patches\n",
    "        self.pos_encoding = get_position_enc(num_patches, n_embd) # adds positional information to patches\n",
    "        self.encoder = nn.Sequential(*[TransformerEncoder(n_embd, num_heads) for _ in range(num_enc_blocks)]) # xL number of encoder blocks\n",
    "        self.mlp_head = MLPHead(n_embd, num_classes) # linear projection head for classification\n",
    "        self.cls_token = nn.Parameter(torch.randn(size=(1, n_embd))) # special token for aggregating global information\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        patch_embd = self.patch_embedding(x)\n",
    "        patch_embd += self.pos_encoding # (B, P, n_embd)\n",
    "        cls_tokens = self.cls_token.expand(size=(64, *self.cls_token.shape)) # (B, 1, n_embd)\n",
    "        tokens = torch.cat([cls_tokens, patch_embd], dim=1) # (B, P+1, n_embd)\n",
    "        representations = self.encoder(tokens) # (B, P+1, n_embd)\n",
    "        cls_patch = representations[:, 0, :] # (B, n_embd)\n",
    "        logits = self.mlp_head(cls_patch) # logits of each class\n",
    "        \n",
    "        if y is None:\n",
    "            return cls_patch, logits # if labels not provided i.e, we are performing inference we want logits\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, y) # performs softmax and then negative log likelihood to find loss\n",
    "            return cls_patch, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 500\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total parameters: 413310\n"
     ]
    }
   ],
   "source": [
    "model = ViT(num_patches, n_embd=100, num_heads=5, num_enc_blocks=3, num_classes=10)\n",
    "optimizer = torch.optim.AdamW(lr=1e-3, params=model.parameters())\n",
    "curr_iter = 0\n",
    "losses = []\n",
    "print(f\"Number of total parameters: {sum([torch.numel(p) for p in model.parameters()])}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Train Loss: 2.3133 Val Loss: 2.3157 Time Taken: 39.698909282684326 seconds\n",
      "Step 500: Train Loss: 1.3670 Val Loss: 1.3853 Time Taken: 38.39172577857971 seconds\n",
      "Step 1000: Train Loss: 1.1904 Val Loss: 1.2501 Time Taken: 40.65891790390015 seconds\n",
      "Step 1500: Train Loss: 1.1726 Val Loss: 1.2068 Time Taken: 45.60847783088684 seconds\n",
      "Step 2000: Train Loss: 1.0730 Val Loss: 1.1166 Time Taken: 47.03711795806885 seconds\n",
      "Step 2500: Train Loss: 0.9726 Val Loss: 1.0774 Time Taken: 46.60057616233826 seconds\n",
      "Step 3000: Train Loss: 1.0026 Val Loss: 1.0889 Time Taken: 54.588908672332764 seconds\n",
      "Step 3500: Train Loss: 0.9218 Val Loss: 1.0282 Time Taken: 48.073211908340454 seconds\n",
      "Step 4000: Train Loss: 0.8356 Val Loss: 1.0178 Time Taken: 45.890907764434814 seconds\n",
      "Step 4500: Train Loss: 0.9478 Val Loss: 1.0647 Time Taken: 49.04400300979614 seconds\n",
      "Step 5000: Train Loss: 0.8133 Val Loss: 0.9606 Time Taken: 49.41571092605591 seconds\n",
      "Step 5500: Train Loss: 0.7645 Val Loss: 0.9744 Time Taken: 45.766302824020386 seconds\n",
      "Step 6000: Train Loss: 0.7722 Val Loss: 0.9559 Time Taken: 45.49068903923035 seconds\n",
      "Step 6500: Train Loss: 0.7138 Val Loss: 0.9738 Time Taken: 46.08877205848694 seconds\n",
      "Step 7000: Train Loss: 0.7162 Val Loss: 0.9153 Time Taken: 51.57953214645386 seconds\n",
      "Step 7500: Train Loss: 0.6608 Val Loss: 0.9306 Time Taken: 45.858580112457275 seconds\n"
     ]
    }
   ],
   "source": [
    "# To train the model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in iter(trainloader):\n",
    "        patches = prepare_images(images) # Get patched images\n",
    "        cls_patch, loss = model(patches, labels) # Get cls patch and loss\n",
    "\n",
    "        if curr_iter % eval_interval == 0:\n",
    "            t1 = time()\n",
    "            losses_split = get_losses() # Get loss on train and validation set\n",
    "            t2 = time()\n",
    "            print(f\"Step {curr_iter}: Train Loss: {losses_split['train']:.4f} Val Loss: {losses_split['val']:.4f} Time Taken: {t2-t1} seconds\")\n",
    "            \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        curr_iter += 1\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, '../SavedModels/vit.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../SavedModels/vit2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Evaluation\n",
    "\n",
    "We will check classification report to see how good the model is at learning, and use metrics like classwise precision, accuracy, recall and the f1 score to see how well our model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73         6\n",
      "           1       0.67      0.67      0.67         3\n",
      "           2       0.50      0.50      0.50         2\n",
      "           3       0.80      0.67      0.73         6\n",
      "           4       0.33      0.50      0.40         6\n",
      "           5       0.71      0.71      0.71         7\n",
      "           6       0.90      0.82      0.86        11\n",
      "           7       0.83      0.71      0.77         7\n",
      "           8       0.83      0.71      0.77         7\n",
      "           9       0.82      1.00      0.90         9\n",
      "\n",
      "    accuracy                           0.73        64\n",
      "   macro avg       0.72      0.70      0.70        64\n",
      "weighted avg       0.76      0.73      0.74        64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for images, labels in iter(valloader):\n",
    "    patches = prepare_images(images) # Get patched images\n",
    "    cls_patch, logits = model(patches) # Get cls logits and loss\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    preds = probs.argmax(dim=-1)\n",
    "    print(classification_report(labels, preds))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding and Exploring Model Predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
