{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT: Vision Transformers\n",
    "\n",
    "This notebook is demonstration and explanantion for a vision transformer. This architecture was created to use transformers which were created for language modelling, on images while at the same time changing as little of the transformer architecture as possible. The notebook will have an implementation of the vision transformer and will break down the purpose of each component in the architecture of the vision transformer. Finally it will be trained to perform a simple classification task on the large Food101 dataset.\n",
    "\n",
    "Reference Paper: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "Prerequisites: \n",
    "- Basic understanding of Neural Networks, Transformers\n",
    "\n",
    "\n",
    "## 1. Introduction:\n",
    "\n",
    "A Vision Transformer: commonly abbreviated to ViT is transformer that has been specially designed to be used for images. Since transformers usually work with text data where they take a sequence of tokens as an input, they cannot be directly used for the task of working with images. Vision transformers take the image as a sequence of patches. These patches are then fed through the network and the learned representations can then be used for any downstream task. \n",
    "\n",
    "The ViT is an encoder only transformer, i.e, it is used to give embeddings of the images that are fed to it, these embeddings are then used for any downstream task, like segmentation, classification e.t.c.\n",
    "\n",
    "\n",
    "## 2. Architecture\n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://viso.ai/wp-content/uploads/2021/09/vision-transformer-vit.png\", width=\"50%\"></center>\n",
    "<br>\n",
    "\n",
    "In the image given above we can see the architecture of the ViT. Lets walk through the components and their purpose: -\n",
    "\n",
    "1. **Patch Embedding:** The patch embedding is responsible for taking each patch of the image and giving an n dimensional embedding. This is similar to how words were embedded in the transformer. The patch embedding is such that after training similar patches will be given similar embeddings.\n",
    "\n",
    "2. **Positional Embedding:** just like in text where we needed to give positional encoding to the words so that they knew where they were lying, similarly we must do so in images as well. Spatial information of the patches is very important and must be preserved, this is because when the patches are fed to a transformer, all patches interact with each other in a global manner, this is unlike convolutional neural networks where filters are used to ensure only pixels in a neighborhood interact to give output, due to the nature of transformers as a result we need to have positional embedding as well. We add this positional embedding to the patch embedding result of each patch to give us the final patch embeddings.\n",
    "\n",
    "3. **Transformer Encoder**: the transformer encoder takes in the embeddings and gives an output which are the learned representations of the patches, this contains useful information like the relationships of the patches with each other, like for example what patch is likely to follow a given patch etc. The components of the transformer encoder are as follows: -\n",
    "\n",
    "    * **Multi-Headed Attention**: attention is the backbone of a transform, it gives us useful information of each individual token. Attention is made up of key, query and value. Where query is for example your patch query, key is for all the other patches in the sequence and their product represents how much your patch is related to to other pathces in the sequence. Finally value is also used, value contains useful information about your specific patch. They all have the same sized embeddings. It is called multi headed attention as there are multiple heads working in parallel which give the final result.\n",
    "\n",
    "    * **MLP**: the multi-layer perceptron is simply a linear projection of the results of our multi headed attention, it is useful for various reasons such as introducing non-linearity, extracting useful features e.t.c.\n",
    "\n",
    "    * **Layer Norm**: layer norm is applied to the input before being fed to mlp and attention, it normalises the values across a column ensuring stability during training.\n",
    "\n",
    "4. **MLP Head**: Finally the mlp head is a projection head that works on the result of the transformer as a whole to perform a downstream task for example classification. The MLP head works only on one patch from the entire sequence of patches that is generated as output of the model. This singular patch can be either a CLS token or it can be created by taking a global average pooling of all the patches.\n",
    "\n",
    "5. **CLS Token**: the CLS token is a special token that is the same dimensionality as patches after their embedding and it is prepended to the sequence of patches before they are fed to the transformer, a CLS token does not have a positional encoding. The idea for a CLS token is that it will act as a global information holder and will contain an aggregate result of the entire image after training. In some implementation it is used as input to the MLP head for the final task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "In this section we will implement the ViT, and with the help of comments and some documentation explain the reasoning behind the code. Lets first start off with downloading the dataset and exploring it as well as creating various functions that we need to prepare our data to be used in the ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Exploration and Preparation:\n",
    "\n",
    "For this section our goal is to perform the following: -\n",
    "\n",
    "1. Download Dataset: CIFAR10 used for this implementation\n",
    "2. Create data transformer, loader\n",
    "3. Create function to prepare an image for it to be used as input to model\n",
    "4. Create function to visualise an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhrao/opt/miniconda3/envs/dl/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import DataLoader, random_split \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Trainset Length: 50000\n",
      "Valset Length: 10000\n"
     ]
    }
   ],
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.Resize(size=(IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.49139968, 0.48215827 ,0.44653124), (0.24703233, 0.24348505, 0.26158768)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transformer)\n",
    "valset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transformer)\n",
    "\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valloader = DataLoader(dataset=valset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "print(f\"Trainset Length: {len(trainset)}\")\n",
    "print(f\"Valset Length: {len(valset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 64, 64])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Get one batch of images for testing purposed\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "print(images.shape) # (B, C, H, W) -> Batch, Channel, Height, Width (this convention will be used)\n",
    "print(labels.shape) # (B,) -> Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_img(data):\n",
    "    # data -> (C, H, W)\n",
    "    img = data.detach().numpy().transpose(1, 2, 0) # to take C to last channel\n",
    "    mean = np.array([0.49139968, 0.48215827 ,0.44653124])\n",
    "    std = np.array([0.24703233, 0.24348505, 0.26158768])\n",
    "    img = std*img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 63.5, 63.5, -0.5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(visualise_img(images[1]))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_h = 4\n",
    "patch_w = 4\n",
    "num_patches = (IMG_HEIGHT // patch_h) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to get image patches\n",
    "\n",
    "def get_patches(imgs):\n",
    "    B, C, H, W = imgs.shape\n",
    "    patches = imgs.unfold(2, patch_h, patch_h).unfold(3, patch_w, patch_w)\n",
    "    patches = patches.contiguous().view(B, C, -1, patch_h, patch_w) # (B, C, P, h, w)\n",
    "    patches = patches.permute(0, 2, 1, 3, 4)\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images(imgs):\n",
    "    patches = get_patches(imgs)\n",
    "    B, P, C, h, w = patches.shape\n",
    "    patches_flat = patches.reshape(B, P, C*h*w)\n",
    "    return patches_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input images: torch.Size([64, 3, 64, 64])\n",
      "Input to ViT: torch.Size([64, 256, 48])\n"
     ]
    }
   ],
   "source": [
    "# Running prepare images function: -\n",
    "\n",
    "print(f\"Input images: {images.shape}\")\n",
    "print(f\"Input to ViT: {prepare_images(images).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works perfectly as we need the ViT to get the input of the images as flattened patches. Now we can get started on the implementation of the actual model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Implementation:\n",
    "\n",
    "For this section our goal is to perform the following: -\n",
    "\n",
    "1. Construct the ViT and all its components\n",
    "2. Train the model on our image data\n",
    "3. Evaluate the model performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 48])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = prepare_images(images)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_losses():\n",
    "    \"\"\"Get loss for train and val split\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        loader = trainloader if split == 'train' else valloader\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        k = 0\n",
    "        for images, labels in iter(loader):\n",
    "            patches = prepare_images(images)\n",
    "            cls_patch, loss = model(patches, labels)\n",
    "            losses[k] = loss.item()\n",
    "            k += 1\n",
    "            if k == eval_iters:\n",
    "                break\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"Single head of attention for transformer\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): built on nn.Module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, head_size) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size) # this represents the other patches think of them as tags\n",
    "        self.query = nn.Linear(n_embd, head_size) # this represents the tokens as queries\n",
    "        self.value = nn.Linear(n_embd, head_size) # information about each token\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Since this is self attention k,q,v are all made from x itself\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) / (k.size(-1) ** 0.5)  \n",
    "        wei = F.softmax(wei, dim=-1) # probability of each patch wrt other patches\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v # adds individual information of each token to the interrelated information found\n",
    "        return out\n",
    "    \n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention for transformer\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        assert n_embd % num_heads == 0, f\"Embedding size: {n_embd} not divisible num of heads: {num_heads}\"\n",
    "        self.head_size = n_embd // num_heads\n",
    "        self.heads = nn.ModuleList([Head(n_embd, self.head_size) for _ in range(num_heads)])\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # Multiple heads give out of head_size which when concatenated become -> n_embd\n",
    "        out = self.c_proj(out)\n",
    "        return out\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-Layer Perceptron within Transformer Encoder Block\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer Encoder Block\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadedAttention(n_embd, num_heads)\n",
    "        self.mlp = MLP(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x)) # this is a residual skip connection, it allows gradients to flow back easily to input\n",
    "        x = x + self.mlp(self.ln2(x)) # you can see the fact that input is added to output of attn and mlp in the diagram given\n",
    "        return x\n",
    "\n",
    "class MLPHead(nn.Module):\n",
    "    \"\"\"MLP Projection Head for classification\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4*n_embd, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "def get_position_enc(num_patch, n_embd):\n",
    "    \"\"\"Gives sinusoidal positional encodings\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "    assert n_embd % 2 == 0, f\"{n_embd} should be even for positional encoding\"\n",
    "\n",
    "    y_pos = torch.arange(0, num_patch, dtype=torch.float).unsqueeze(1)\n",
    "    x_pos = torch.arange(0, num_patch, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, n_embd // 2, 2).float() * (-torch.log(torch.tensor(1000.0)) / (n_embd // 2)))\n",
    "\n",
    "    pos_encoding = torch.zeros((num_patch, n_embd))\n",
    "    pos_encoding[:, 0::4] = torch.sin(y_pos * div_term)\n",
    "    pos_encoding[:, 1::4] = torch.cos(y_pos * div_term)\n",
    "    pos_encoding[:, 2::4] = torch.sin(x_pos * div_term)\n",
    "    pos_encoding[:, 3::4] = torch.cos(x_pos * div_term)\n",
    "\n",
    "    return pos_encoding\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"Vision Transfomer\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_patches, n_embd, num_heads, num_enc_blocks, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.patch_embedding = nn.Linear(3*patch_h*patch_w, n_embd) # embeds the patches\n",
    "        self.pos_encoding = get_position_enc(num_patches, n_embd) # adds positional information to patches\n",
    "        self.encoder = nn.Sequential(*[TransformerEncoder(n_embd, num_heads) for _ in range(num_enc_blocks)]) # xL number of encoder blocks\n",
    "        self.mlp_head = MLPHead(n_embd, num_classes) # linear projection head for classification\n",
    "        self.cls_token = nn.Parameter(torch.randn(size=(1, n_embd))) # special token for aggregating global information\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        patch_embd = self.patch_embedding(x)\n",
    "        patch_embd += self.pos_encoding # (B, P, n_embd)\n",
    "        cls_tokens = self.cls_token.expand(size=(64, *self.cls_token.shape)) # (B, 1, n_embd)\n",
    "        tokens = torch.cat([cls_tokens, patch_embd], dim=1) # (B, P+1, n_embd)\n",
    "        representations = self.encoder(tokens) # (B, P+1, n_embd)\n",
    "        cls_patch = representations[:, 0, :] # (B, n_embd)\n",
    "        logits = self.mlp_head(cls_patch) # logits of each class\n",
    "        \n",
    "        if y is None:\n",
    "            return cls_patch, logits # if labels not provided i.e, we are performing inference we want logits\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, y) # performs softmax and then negative log likelihood to find loss\n",
    "            return cls_patch, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 100\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766330\n"
     ]
    }
   ],
   "source": [
    "model = ViT(num_patches, n_embd=120, num_heads=6, num_enc_blocks=4, num_classes=10)\n",
    "optimizer = torch.optim.AdamW(lr=1e-3, params=model.parameters())\n",
    "curr_iter = 0\n",
    "losses = []\n",
    "print(f\"Number of total parameters: {sum([torch.numel(p) for p in model.parameters()])}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1600: Train Loss: 1.0857 Val Loss: 1.1368 Time Taken: 64.53267502784729 seconds\n",
      "Step 1700: Train Loss: 1.0438 Val Loss: 1.1189 Time Taken: 67.58067202568054 seconds\n",
      "Step 1800: Train Loss: 1.0250 Val Loss: 1.1268 Time Taken: 67.75099301338196 seconds\n",
      "Step 1900: Train Loss: 1.1154 Val Loss: 1.1736 Time Taken: 70.53208112716675 seconds\n",
      "Step 2000: Train Loss: 1.0241 Val Loss: 1.1055 Time Taken: 66.08223700523376 seconds\n",
      "Step 2100: Train Loss: 0.9827 Val Loss: 1.0892 Time Taken: 70.70575213432312 seconds\n",
      "Step 2200: Train Loss: 1.0413 Val Loss: 1.1132 Time Taken: 71.629075050354 seconds\n",
      "Step 2300: Train Loss: 1.0309 Val Loss: 1.1245 Time Taken: 71.4351270198822 seconds\n",
      "Step 2400: Train Loss: 0.9636 Val Loss: 1.0517 Time Taken: 71.52533483505249 seconds\n",
      "Step 2500: Train Loss: 0.9512 Val Loss: 1.0730 Time Taken: 72.67449021339417 seconds\n",
      "Step 2600: Train Loss: 0.8985 Val Loss: 1.0205 Time Taken: 72.13788199424744 seconds\n",
      "Step 2700: Train Loss: 0.9561 Val Loss: 1.0662 Time Taken: 75.42447185516357 seconds\n",
      "Step 2800: Train Loss: 0.9256 Val Loss: 1.0190 Time Taken: 73.04910087585449 seconds\n",
      "Step 2900: Train Loss: 0.8836 Val Loss: 1.0311 Time Taken: 76.34074687957764 seconds\n",
      "Step 3000: Train Loss: 0.9010 Val Loss: 1.0209 Time Taken: 74.14023494720459 seconds\n",
      "Step 3100: Train Loss: 0.9260 Val Loss: 1.0758 Time Taken: 75.04087400436401 seconds\n",
      "Step 3200: Train Loss: 0.8347 Val Loss: 0.9855 Time Taken: 76.47094917297363 seconds\n",
      "Step 3300: Train Loss: 0.8644 Val Loss: 1.0177 Time Taken: 80.82806301116943 seconds\n",
      "Step 3400: Train Loss: 0.8524 Val Loss: 1.0051 Time Taken: 77.16219282150269 seconds\n",
      "Step 3500: Train Loss: 0.8219 Val Loss: 1.0086 Time Taken: 73.28454899787903 seconds\n",
      "Step 3600: Train Loss: 0.7980 Val Loss: 1.0000 Time Taken: 76.24947690963745 seconds\n",
      "Step 3700: Train Loss: 0.8735 Val Loss: 1.0148 Time Taken: 77.14635872840881 seconds\n",
      "Step 3800: Train Loss: 0.7917 Val Loss: 0.9776 Time Taken: 77.3626172542572 seconds\n",
      "Step 3900: Train Loss: 0.7666 Val Loss: 0.9391 Time Taken: 74.3784236907959 seconds\n",
      "Step 4000: Train Loss: 0.7572 Val Loss: 0.9616 Time Taken: 73.93057775497437 seconds\n",
      "Step 4100: Train Loss: 0.8250 Val Loss: 1.0343 Time Taken: 77.10351896286011 seconds\n",
      "Step 4200: Train Loss: 0.7938 Val Loss: 0.9695 Time Taken: 74.71798086166382 seconds\n",
      "Step 4300: Train Loss: 0.7430 Val Loss: 0.9552 Time Taken: 77.18114566802979 seconds\n",
      "Step 4400: Train Loss: 0.7938 Val Loss: 0.9822 Time Taken: 73.73061299324036 seconds\n",
      "Step 4500: Train Loss: 0.7666 Val Loss: 0.9584 Time Taken: 76.58653688430786 seconds\n",
      "Step 4600: Train Loss: 0.7962 Val Loss: 0.9788 Time Taken: 74.01619911193848 seconds\n",
      "Step 4700: Train Loss: 0.7363 Val Loss: 0.9640 Time Taken: 74.67395687103271 seconds\n",
      "Step 4800: Train Loss: 0.7152 Val Loss: 0.9564 Time Taken: 74.09466695785522 seconds\n",
      "Step 4900: Train Loss: 0.7816 Val Loss: 0.9837 Time Taken: 75.79062676429749 seconds\n",
      "Step 5000: Train Loss: 0.6831 Val Loss: 0.9144 Time Taken: 73.88268494606018 seconds\n",
      "Step 5100: Train Loss: 0.6795 Val Loss: 0.8916 Time Taken: 75.02462673187256 seconds\n",
      "Step 5200: Train Loss: 0.6823 Val Loss: 0.9245 Time Taken: 74.30553698539734 seconds\n",
      "Step 5300: Train Loss: 0.6717 Val Loss: 0.9133 Time Taken: 74.03612112998962 seconds\n",
      "Step 5400: Train Loss: 0.7814 Val Loss: 1.0119 Time Taken: 101.72090601921082 seconds\n"
     ]
    }
   ],
   "source": [
    "# To train the model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in iter(trainloader):\n",
    "        patches = prepare_images(images) # Get patched images\n",
    "        cls_patch, loss = model(patches, labels) # Get cls patch and loss\n",
    "\n",
    "        if curr_iter % eval_interval == 0:\n",
    "            t1 = time()\n",
    "            losses_split = get_losses() # Get loss on train and validation set\n",
    "            t2 = time()\n",
    "            print(f\"Step {curr_iter}: Train Loss: {losses_split['train']:.4f} Val Loss: {losses_split['val']:.4f} Time Taken: {t2-t1} seconds\")\n",
    "            \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        curr_iter += 1\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding and Exploring Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
